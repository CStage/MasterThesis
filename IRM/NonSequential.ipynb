{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive\n",
        "For easy access to datasets"
      ],
      "metadata": {
        "id": "Wig3P2tUSYPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woCAiXwCO8UN",
        "outputId": "72122a16-892f-4208-b91f-1b753b47572c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "oP2r8E_xSfju"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBTmatDSV7j5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c421481-8026-4f41-c150-765fe1ca90a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import h5py\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import sys\n",
        "import math\n",
        "import scipy\n",
        "import statistics\n",
        "csv.field_size_limit(sys.maxsize)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Settings:** Paths, version etc.\n",
        "Specify if Christian or Philip here"
      ],
      "metadata": {
        "id": "bwPCYCT3emx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Put name here!\n",
        "name = \"christian\"\n",
        "\n",
        "#Dataset version\n",
        "version = \"resnet\"\n",
        "\n",
        "if name.lower() == \"christian\":\n",
        "  checkpoint_path = r\"/content/drive/MyDrive/ML models/Checkpoints/IRM\"\n",
        "  train_path = r\"/content/drive/MyDrive/Data/Train.hdf5\"\n",
        "  dev_path = r\"/content/drive/MyDrive/Data/Dev.hdf5\"\n",
        "  test_path = r\"/content/drive/MyDrive/Data/Test.hdf5\"\n",
        "  csv_path = r\"/content/drive/MyDrive/Data/CSV_Data\"\n",
        "  image_path = r\"/content/drive/MyDrive/Data/Images\"\n",
        "  seq_lengths_path = r\"/content/drive/MyDrive/Data/seq_lengths.hdf5\"\n",
        "elif name.lower() == \"philip\":\n",
        "  checkpoint_path = r\"/content/drive/MyDrive/ITU/Software Design - Kandidat/Master Thesis/ML models/Checkpoints/IRM\"\n",
        "  train_path = r\"/content/drive/MyDrive/ITU/Software Design - Kandidat/Master Thesis/Data/Train.hdf5\"\n",
        "  dev_path = r\"/content/drive/MyDrive/ITU/Software Design - Kandidat/Master Thesis/Data/Dev.hdf5\"\n",
        "  test_path = r\"/content/drive/MyDrive/ITU/Software Design - Kandidat/Master Thesis/Data/Test.hdf5\"\n",
        "  csv_path = r\"/content/drive/MyDrive/ITU/Software Design - Kandidat/Master Thesis/Data/CSV_Data\"\n",
        "  image_path = r\"/content/drive/MyDrive/ITU/Software Design - Kandidat/Master Thesis/Data/Images\"\n",
        "  seq_lengths_path = r\"/content/drive/MyDrive/ITU/Software Design - Kandidat/Master Thesis/Data/seq_lengths.hdf5\"\n",
        "else:\n",
        "  raise Exception(\"Invalid name for path - use 'christian' or 'philip'\")"
      ],
      "metadata": {
        "id": "q-paJ85xerTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Device"
      ],
      "metadata": {
        "id": "co-q_h3yS6Oj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOtZpR1Uyp33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d256e40a-2b88-4b0b-d3da-d0d3f8bd86f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup:** Dataset Class\n",
        "Note, this is slightly different from the tutorial that this implementation is based on as our dataset is in the *HDF5*-format"
      ],
      "metadata": {
        "id": "3tTs2-3cShhS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Fb4lpUCfPQf"
      },
      "outputs": [],
      "source": [
        "# Current version used for testing the set up\n",
        "\n",
        "class FiskeSet_test(Dataset):\n",
        "  def __init__(self, anchors, positives, seed, amount, transform=None, version=\"vanilla\"):\n",
        "    random.seed(seed)\n",
        "    if amount <=1:\n",
        "      raise Exception(\"Amount of samples must be greater than 1 or negatives cannot be fetched.\")\n",
        "\n",
        "\n",
        "    self.anchors = anchors\n",
        "    self.positives = positives\n",
        "    self.transform = transform\n",
        "    self.amount = amount\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.amount\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    anchor = self.anchors[idx]\n",
        "    positive = self.positives[idx]\n",
        "\n",
        "    negative = self.anchors[random.choice(range(self.amount))]\n",
        "\n",
        "    while list(anchor) == list(negative):\n",
        "      negative = self.anchors[random.choice(range(self.amount))]\n",
        "\n",
        "    if self.transform:\n",
        "      if version != \"vanilla\":\n",
        "        positive = Image.fromarray(positive)\n",
        "      positive = self.transform(positive)\n",
        "\n",
        "    return anchor, positive, negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYrPz1O8qMvT"
      },
      "outputs": [],
      "source": [
        "# Generic version used for the entire dataset (once ready)\n",
        "\n",
        "class FiskeSet(Dataset):\n",
        "  def __init__(self, anchors, positives, seed, transform=None, version = \"vanilla\"):\n",
        "    random.seed(seed)\n",
        "    \n",
        "    self.anchors = anchors\n",
        "    self.positives = positives\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.positives)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    anchor = self.anchors[idx]\n",
        "    positive = self.positives[idx]\n",
        "\n",
        "    negative = random.choice(self.anchors)\n",
        "\n",
        "    while list(anchor) == list(negative):\n",
        "      negative = random.choice(self.anchors)\n",
        "\n",
        "    if self.transform:\n",
        "      if version != \"vanilla\":\n",
        "        positive = Image.fromarray(positive)\n",
        "      positive = self.transform(positive)\n",
        "\n",
        "    return anchor, positive, negative"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup:** Data Loading"
      ],
      "metadata": {
        "id": "U5NJSe5fStsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Images need to be transformed in accordance with the specifications of the version\n",
        "\n",
        "if version == \"resnet\":\n",
        "  transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "elif version == \"vanilla\":\n",
        "  transform = transforms.Compose(\n",
        "      [transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "else:\n",
        "  raise Exception(\"Please input a valid cnn-model\")\n",
        "\n",
        "def generate_genres_tensor(genres_vectors):\n",
        "  \"\"\"\n",
        "  Takes a bunch of vectors (numpy arrays) of genre-names and creates one-hot vectors\n",
        "  based on them. The resulting tensor can be concatenated to anchors using .cat.\n",
        "  \"\"\"\n",
        "  genres_master = [\"Blues\", \"Classical\", \"Electronic\", \"Folk World & Country\", \"Funk / Soul\", \"Hip Hop\", \"Jazz\", \"Latin\",\n",
        "              \"Pop\", \"Reggae\", \"Rock\"]\n",
        "  genres_tensor = []\n",
        "  for idx, genres in enumerate(genres_vectors):\n",
        "    list_vector = []\n",
        "    if \"Folk\" in genres:\n",
        "      genres = genres.replace(\"Folk, World,\", \"Folk World\")\n",
        "    genres = genres.split(\", \")\n",
        "    for i, g in enumerate(genres_master):\n",
        "      if g in genres:\n",
        "        list_vector.append(1)\n",
        "      else:\n",
        "        list_vector.append(0)\n",
        "    genres_tensor.append(torch.tensor(list_vector))\n",
        "  genres_tensor = torch.stack(tuple(genres_tensor))\n",
        "  return genres_tensor\n",
        "\n",
        "#One hot encoding for years. Needs review.\n",
        "def generate_years_tensor(years_vectors):\n",
        "\n",
        "  years_master = list(range(100))\n",
        "\n",
        "  years_tensor = []\n",
        "  for idx, year in enumerate(years_vectors):\n",
        "    list_vector = []\n",
        "    for y in years_master:\n",
        "      if (int(year[-2:]) == int(y)):\n",
        "        list_vector.append(1)\n",
        "      else:\n",
        "        list_vector.append(0)\n",
        "    years_tensor.append(torch.tensor(list_vector))\n",
        "  years_tensor = torch.stack(tuple(years_tensor))\n",
        "  return years_tensor\n",
        "\n",
        "def data_normalization(data):\n",
        "\n",
        "  minimum = np.amin(data)\n",
        "  maximum = np.amax(data)\n",
        "  for i in range(len(data)):\n",
        "    data[i] = (data[i]-minimum)/(maximum-minimum)\n",
        "  \n",
        "  return torch.tensor(data)\n",
        "\n",
        "def create_dataset(path, input_attributes, seed, amount = None, debug = False, version = \"vanilla\"):\n",
        "  h = h5py.File(path)\n",
        "\n",
        "  svd_attributes = list(h[\"Single Value Data\"].attrs[\"column_names\"])\n",
        "  md_attributes = list(h[\"Metadata\"].attrs[\"column_names\"])\n",
        "\n",
        "\n",
        "  svd_idx_list = []\n",
        "  tensor_lyf = []\n",
        "  if debug:\n",
        "    random.seed(seed)\n",
        "    rands = random.sample(range(h[\"Single Value Data\"].shape[0]), amount)\n",
        "    rands.sort()\n",
        "    images = h[\"Images\"][rands]\n",
        "    for att in input_attributes:\n",
        "      if \"Genres\" == att:\n",
        "        genres_vectors = h[\"Metadata\"].asstr()[rands, md_attributes.index(\"Genres\")]\n",
        "        genres_tensor = generate_genres_tensor(genres_vectors)\n",
        "        tensor_lyf.append(genres_tensor)\n",
        "        #anchors = torch.cat((anchors, genres_tensor), dim=1)\n",
        "      elif \"Year\" == att:\n",
        "        years_vectors = h[\"Metadata\"].asstr()[rands, md_attributes.index(\"Year\")]\n",
        "        years_tensor = generate_years_tensor(years_vectors)\n",
        "        tensor_lyf.append(years_tensor)\n",
        "        #anchors = torch.cat((anchors, years_tensor), dim=1)\n",
        "      elif \"_norm\" in att:\n",
        "        clean_att = att.replace(\"_norm\", \"\")\n",
        "        idx = svd_attributes.index(clean_att)\n",
        "        att_vectors = h[\"Single Value Data\"][rands, idx]\n",
        "        att_tensor = data_normalization(att_vectors)\n",
        "        att_tensor = att_tensor[:, None]\n",
        "        tensor_lyf.append(att_tensor)\n",
        "        #anchors = torch.cat((anchors, att_tensor), dim=1)\n",
        "      else:\n",
        "        svd_idx_list.append(svd_attributes.index(att))\n",
        "    for svd in svd_idx_list:\n",
        "      addition_tensor = h[\"Single Value Data\"][rands, svd]\n",
        "      addition_tensor = addition_tensor[:, None]\n",
        "      tensor_lyf.append(torch.tensor(addition_tensor))\n",
        "    anchors = torch.cat(tuple(tensor_lyf), dim=1)\n",
        "    print(\"Anchors SHAPE:\", anchors.shape)\n",
        "    dataset = FiskeSet_test(anchors, images, seed, amount, transform, version)\n",
        "    \n",
        "  else:\n",
        "    images = h[\"Images\"]\n",
        "    for att in input_attributes:\n",
        "      if \"Genres\" == att:\n",
        "        genres_vectors = h[\"Metadata\"].asstr()[:, md_attributes.index(\"Genres\")]\n",
        "        genres_tensor = generate_genres_tensor(genres_vectors)\n",
        "        tensor_lyf.append(genres_tensor)\n",
        "      elif \"Year\" == att:\n",
        "        years_vectors = h[\"Metadata\"].asstr()[:, md_attributes.index(\"Year\")]\n",
        "        years_tensor = generate_years_tensor(years_vectors)\n",
        "        tensor_lyf.append(years_tensor)\n",
        "      elif \"_norm\" in att:\n",
        "        clean_att = att.replace(\"_norm\", \"\")\n",
        "        idx = svd_attributes.index(clean_att)\n",
        "        att_vectors = h[\"Single Value Data\"][:, idx]\n",
        "        att_tensor = data_normalization(att_vectors)\n",
        "        att_tensor = att_tensor[:, None]\n",
        "        tensor_lyf.append(att_tensor)\n",
        "      else:\n",
        "        svd_idx_list.append(svd_attributes.index(att))\n",
        "    for svd in svd_idx_list:\n",
        "      addition_tensor = h[\"Single Value Data\"][:, svd]\n",
        "      addition_tensor = addition_tensor[:, None]\n",
        "      tensor_lyf.append(torch.tensor(addition_tensor))\n",
        "    anchors = torch.cat(tuple(tensor_lyf), dim=1)\n",
        "    print(\"Anchors SHAPE:\", anchors.shape)\n",
        "\n",
        "    dataset = FiskeSet(anchors, images, seed, transform, version)\n",
        "    # print(\"Hello?\", genres_tensor.shape, anchors.shape)\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "7nAmByBwFCJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup:** Image conversion\n"
      ],
      "metadata": {
        "id": "cAThTrWCyfxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_img(rgb_matrix):\n",
        "  img = rgb_matrix.permute(1, 2, 0)\n",
        "  img = np.array(img)\n",
        "  plt.imshow(img)\n",
        "\n",
        "def show_x_images(imgs):\n",
        "  fig = plt.figure(figsize=(50, 30))\n",
        "\n",
        "  x = len(imgs)\n",
        "  # setting values to rows and column variables\n",
        "  if x % 2 == 0:\n",
        "    rows = x/2\n",
        "    columns = x/2\n",
        "  else:\n",
        "    rows = (x+1)/2\n",
        "    columns = (x+1)/2\n",
        "\n",
        "  for i in range(1, x+1):\n",
        "    # Adds a subplot at the 1st position\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "\n",
        "    # showing image\n",
        "    show_img(imgs[i-1])\n",
        "    plt.axis('off')\n",
        "    if i == 1:\n",
        "      plt.title(\"ANCHOR\")\n",
        "    else:\n",
        "      plt.title(i)\n",
        "    \n",
        "def show_images_advanced(img_dict, save = False):\n",
        "  fig = plt.figure(figsize=(20, 20))\n",
        "  plt.tight_layout()\n",
        "  x = len(img_dict)\n",
        "  postfix = str(x) + \"_images_\" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "  if x % 2 == 0:\n",
        "    rows = x//2\n",
        "    columns = x//2\n",
        "  else:\n",
        "    rows = (x+1)//2\n",
        "    columns = rows\n",
        "\n",
        "  idx = 0\n",
        "  for key, value in img_dict.items():\n",
        "    if key == \"Anchor\":\n",
        "      plt.subplot2grid((rows, columns), (0, columns//2-1), rowspan=2, colspan=2)\n",
        "      show_img(value)\n",
        "      plt.axis('off')\n",
        "      idx += columns*2\n",
        "\n",
        "    else:\n",
        "      plt.subplot2grid((rows, columns), (idx//rows, idx%columns))\n",
        "      show_img(value)\n",
        "      plt.axis('off')\n",
        "      idx += 1\n",
        "    plt.title(key)\n",
        "\n",
        "  if save:\n",
        "    plt.savefig(f\"{image_path}/{postfix}.pdf\", dpi=300, bbox_inches=\"tight\")\n",
        "    "
      ],
      "metadata": {
        "id": "HUNRjqPhynrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Settings:** Training initialization\n",
        "Options are specified here"
      ],
      "metadata": {
        "id": "NPl7rVdFa1S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_attributes = [\"energy\", \"danceability\", \"speechiness\", \"acousticness\", \"instrumentalness\",\n",
        "                    \"liveness\", \"valence\", \"Year\", \"Genres\", \"tempo_norm\", \"loudness_norm\", \"key_norm\",\n",
        "                    \"mode\"]\n",
        "genre_dim = 10\n",
        "year_dim = 99\n",
        "\n",
        "in_dim = len(input_attributes) + genre_dim + year_dim\n",
        "out_dim = 1000\n",
        "print(\"in_dim:\", in_dim)\n",
        "seed = 42\n",
        "batch_size = 16\n",
        "##Change if in Debug mode\n",
        "amount = 10_000\n",
        "\n",
        "\n",
        "train_set = create_dataset(train_path, input_attributes, seed, amount, True, version)\n",
        "show_img(train_set[0][1])\n",
        "#dev_set = create_dataset(dev_path, input_attributes)\n",
        "#test_set = create_dataset(test_path, input_attributes)\n",
        "amount = len(train_set)\n",
        "\n",
        "trainloader = DataLoader(train_set, shuffle = True, num_workers = 2, batch_size=batch_size)\n",
        "#devloader = DataLoader(dev_set, shuffle = True, num_workers = 2, batch_size=batch_size)\n",
        "#testloader = DataLoader(test_set, shuffle = True, num_workers = 2, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "BARU4KFZa7rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Class\n",
        "Output mapped between -1 to 1 with tanh"
      ],
      "metadata": {
        "id": "OlPKYVIyS8Kc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqsgx982mHdF"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3,6,5)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(6,16,5)\n",
        "    self.fc1 = nn.Linear(2704, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 128)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.leaky_relu(self.conv1(x)))\n",
        "    x = self.pool(F.leaky_relu(self.conv2(x)))\n",
        "    x = torch.flatten(x,1)\n",
        "    x = F.leaky_relu(self.fc1(x))\n",
        "    x = F.leaky_relu(self.fc2(x)) \n",
        "    x = torch.tanh(self.fc3(x))\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep CNN Class"
      ],
      "metadata": {
        "id": "Hn3airHrdjNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Deep_CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3,6,5)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(6,16,5)\n",
        "    self.fc1 = nn.Linear(2704, 256)\n",
        "    self.fc2 = nn.Linear(256, 256)\n",
        "    self.fc3 = nn.Linear(256, 128)\n",
        "    self.fc4 = nn.Linear(128, 84)\n",
        "    self.fc5 = nn.Linear(84, 128)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.leaky_relu(self.conv1(x)))\n",
        "    x = self.pool(F.leaky_relu(self.conv2(x)))\n",
        "    x = torch.flatten(x,1)\n",
        "    x = F.leaky_relu(self.fc1(x))\n",
        "    x = F.leaky_relu(self.fc2(x))\n",
        "    x = F.leaky_relu(self.fc3(x))\n",
        "    x = F.leaky_relu(self.fc4(x)) \n",
        "    x = torch.tanh(self.fc5(x))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "KUXiXD-ddi_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet"
      ],
      "metadata": {
        "id": "wFY4jtbaZ6Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnc3fpOrZ_Kq",
        "outputId": "20d54885-1f0c-4660-8ab5-b6ebc3727fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Class\n",
        "\n",
        "This is a setup of the multilayer perceptron used for encoding.\n",
        "Output mapped between -1 to 1 with tanh"
      ],
      "metadata": {
        "id": "_b1rB6FH1CkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_dim, 32)\n",
        "    self.fc2 = nn.Linear(32, 64)\n",
        "    self.fc3 = nn.Linear(64, out_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = F.leaky_relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.leaky_relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = torch.tanh(x)\n",
        "    \n",
        "    return x"
      ],
      "metadata": {
        "id": "_-lZUYdu1Iw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep MLP Class"
      ],
      "metadata": {
        "id": "-_bEnOkPdsqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Deep_MLP(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_dim, 32)\n",
        "    self.fc2 = nn.Linear(32, 32)\n",
        "    self.fc3 = nn.Linear(32, 64)\n",
        "    self.fc4 = nn.Linear(64, 64)\n",
        "    self.fc5 = nn.Linear(64, out_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.leaky_relu(self.fc1(x))\n",
        "    x = F.leaky_relu(self.fc2(x))\n",
        "    x = F.leaky_relu(self.fc3(x))\n",
        "    x = F.leaky_relu(self.fc4(x)) \n",
        "    x = torch.tanh(self.fc5(x))\n",
        "    \n",
        "    return x"
      ],
      "metadata": {
        "id": "GVi0bvo3dsbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss and Optimizer"
      ],
      "metadata": {
        "id": "4RY2uLI4TdCs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw8O0kmSndj2"
      },
      "outputs": [],
      "source": [
        "criterion = nn.TripletMarginLoss()\n",
        "params = list(cnn.parameters())\n",
        "params.extend(mlp.parameters())\n",
        "optimizer = optim.Adam(params, lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Settings:** Training\n",
        "\n",
        "Specify epochs and which models to train"
      ],
      "metadata": {
        "id": "J2WuLs-PT9vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp.train()\n",
        "cnn.train()\n",
        "\n",
        "losses = []\n",
        "epochs = 40\n",
        "\n",
        "#Change to save checkpoints\n",
        "enable_checkpoints = True"
      ],
      "metadata": {
        "id": "y-h4XukUT89V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Routine"
      ],
      "metadata": {
        "id": "CCrgc3XGTh_E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE21tU8Wn35z"
      },
      "outputs": [],
      "source": [
        "if enable_checkpoints:\n",
        "  time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "  folder_path = checkpoint_path + \"/\" + str(time)\n",
        "  os.mkdir(folder_path)\n",
        "  os.mkdir(folder_path + \"/mlp\")\n",
        "  os.mkdir(folder_path + \"/cnn\")\n",
        "\n",
        "  with open(folder_path + r\"/model_settings.txt\", \"w\") as current_settings:\n",
        "    current_settings.write(f\"ATTRIBUTES: {input_attributes} \\nAMOUNT: {amount} \\nSEED: {seed} \\nBATCH SIZE: {batch_size}\")\n",
        "\n",
        "  with open(folder_path + r\"/epoch_losses.csv\", \"w\", encoding=\"UTF8\", newline=\"\") as epoch_losses:\n",
        "    writer = csv.writer(epoch_losses, delimiter=\";\")\n",
        "    writer.writerow([\"EPOCH\", \"LOSS PER SAMPLE\"])\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "\n",
        "  running_loss = 0.0\n",
        "  #print(f\"Epoch: {epoch}\")\n",
        "  for i, data in enumerate(trainloader):\n",
        "\n",
        "    anchors, positives, negatives = data\n",
        "\n",
        "    anchors = anchors.to(device)\n",
        "    anchors = anchors.float()\n",
        "    positives = positives.to(device)\n",
        "    positives = positives.float()\n",
        "    negatives = negatives.to(device)\n",
        "    negatives = negatives.float()\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    anchors = mlp(anchors)\n",
        "    negatives = mlp(negatives)\n",
        "    positives = cnn(positives)\n",
        "\n",
        "    loss = criterion(anchors, positives, negatives)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "  \n",
        "  print(f\"\\nEpoch Loss: {running_loss/(amount/batch_size)}\")\n",
        "  losses.append(running_loss/(amount/batch_size))\n",
        "\n",
        "  if enable_checkpoints:\n",
        "    torch.save(mlp.state_dict(), f\"{folder_path}/mlp/MLP_epoch_{epoch}\")\n",
        "    torch.save(cnn.state_dict(), f\"{folder_path}/cnn/CNN_epoch_{epoch}\")\n",
        "\n",
        "    with open(folder_path + r\"/epoch_losses.csv\", \"a\", encoding=\"UTF8\", newline=\"\") as epoch_losses:\n",
        "      writer = csv.writer(epoch_losses, delimiter=\";\")\n",
        "      writer.writerow([epoch, running_loss/(amount/batch_size)])\n",
        "\n",
        "print(\"Finished training!\")\n",
        "print(losses)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Q2kSBxkS1eNN",
        "wFY4jtbaZ6Wo",
        "4RY2uLI4TdCs",
        "J2WuLs-PT9vv",
        "CCrgc3XGTh_E",
        "BJ-Ar7YfGlti",
        "e4sIWdZ6Kb4c",
        "3ZCIW5AcXLNY",
        "TxX-3Vb2Jf_G"
      ],
      "name": "ImageRetrievalModel Training.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}